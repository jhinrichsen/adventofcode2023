= Advent of code 2023
:doctype: book
:toc:

image:https://godoc.org/gitlab.com/jhinrichsen/adventofcode2023?status.svg["godoc", link="https://godoc.org/gitlab.com/jhinrichsen/adventofcode2023"]
image:https://goreportcard.com/badge/gitlab.com/jhinrichsen/adventofcode2023["Go report card", link="https://goreportcard.com/report/gitlab.com/jhinrichsen/adventofcode2023"]
image:https://gitlab.com/jhinrichsen/adventofcode2023/badges/main/pipeline.svg[link="https://gitlab.com/jhinrichsen/adventofcode2023/-/commits/main",title="pipeline status"]
image:https://gitlab.com/jhinrichsen/adventofcode2023/badges/main/coverage.svg[link="https://gitlab.com/jhinrichsen/adventofcode2023/badges/main/coverage.svg",title="coverage report"]


[quote, me]
Same procedure as every year, Eric.

image::static/gitlab-history.png[]

My take on https://adventofcode.com/2023/ in Go. As usual, i don't particularly
care if i provide my solutions _fast_, i try to be _correct_ on the first
answer, and care for being runtime efficient.
All puzzles are backed by unit testing the examples and the puzzles.
Results are hard coded into the unit tests, so you might not want to peek at `_test.go` files.

== Environment

- Go 1.22, 1.23
- vim, vim-go, gopls, fed by an HHKB
- VisualStudio Code for debugging
- Fedora up to 41 @ Framework 16" (AMD Ryzen 7 7840HS w/ Radeon 780M Graphics)
- Fedora up to 41 @ custom build (AMD Ryzen 5 3400G on a Gigabyte B450)
- OSX @ 16" Macbook Pro 2019 (Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz)
- Prompts: Tabnine, ChatGPT, Gemini

I'd love to try one of those Risc-V boards but can't decide between
https://github.com/openwch/ch32v003[a bare metal 48 MHz]
- probably requiring tinygo - or a full fledge
https://www.sifive.com/cores/performance-p870-p870a[multicore GHz monster].

== Project structure

Each day lives separately in a `day{{.n}}.go` and `day{{.n}}_test.go` file.
Unit test data, both examples and puzzle input, is in
`testdata/day{{.n}}.txt`, and `testdata/day{{.n}}_example.txt`.

== Overview

Number of tries for a correct answer:

|===
| Day | Part 1 | Part 2
| 1   |   1    |  3
| 2   |   1    |  1
| 3   |   3    |  2
| 4   |   1    |  1
| 5   |   1    |  2
| 6   |   1    |  1
| 7   |   1    |  2
| 8   |   1    |  2
| 9   |   1    |  1
| 10  |   1    |  1
| 11  |   1    |  1
| 12  |   1    |  1
| 13  |   1    |  1
| 14  |   1    |  1
| 15  |   1    |  1
| 16  |   1    |  1
| 17  |   1    |  1
| 18  |   1    |  1
| 19  |        |
| 20  |        |
| 21  |        |
| 22  |        |
| 23  |        |
| 24  |        |
| 25  |        |
|===

== Day 17: Clumsy Crucible

=== Performance Optimization

Day 17 was optimized by replacing `container/heap` with a custom generic heap implementation.

==== Baseline (b0)

Initial implementation using `container/heap` with day-specific `priorityQueue17` type wrapping `heap.Interface`.

==== Failed Optimization (b1, b2)

Attempted generic implementations still using `container/heap` were 18-22% slower due to `heap.Interface` overhead, not generics themselves.

==== Successful Optimization (b3)

Custom generic heap implementation without `container/heap`, using Go's monomorphization for zero-abstraction cost.

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ day17_bench_b0.txt │         day17_bench_b3.txt          │
              │       sec/op       │   sec/op     vs base                │
Day17Part1-16          309.3m ± 9%   269.6m ± 5%  -12.86% (p=0.000 n=10)
Day17Part2-16         1135.0m ± 6%   907.5m ± 5%  -20.04% (p=0.000 n=10)
geomean                592.5m        494.6m       -16.53%

              │ day17_bench_b0.txt │          day17_bench_b3.txt          │
              │        B/op        │     B/op      vs base                │
Day17Part1-16         99.85Mi ± 0%   90.18Mi ± 0%   -9.69% (p=0.000 n=10)
Day17Part2-16         251.9Mi ± 0%   225.4Mi ± 0%  -10.49% (p=0.000 n=10)
geomean               158.6Mi        142.6Mi       -10.09%

              │ day17_bench_b0.txt │         day17_bench_b3.txt         │
              │     allocs/op      │  allocs/op   vs base               │
Day17Part1-16          1.017M ± 0%   1.017M ± 0%       ~ (p=0.424 n=10)
Day17Part2-16          2.953M ± 0%   2.953M ± 0%  -0.00% (p=0.001 n=10)
geomean                1.733M        1.733M       -0.00%
----

**Key Improvements:**

* **Part 1**: 13% faster (309ms → 270ms)
* **Part 2**: 20% faster (1135ms → 908ms)
* **Memory**: 10% less (99.8Mi → 90.2Mi)

**Conclusion:**

The overhead was from `container/heap`'s `heap.Interface`, not Go generics. A proper generic heap using monomorphization outperforms hand-written type-specific code. Go generics have zero abstraction cost when designed correctly - the key is avoiding unnecessary interface indirection.

== Day 18: Lavaduct Lagoon

Calculate the area of a lagoon by tracing its perimeter with dig instructions.

**Algorithm:**

* **Shoelace formula** to compute polygon area from vertices
* **Pick's theorem** to convert area and perimeter to total interior + boundary points
* Formula: `total = area + perimeter/2 + 1`

**Part 1:** Use direction and distance from instruction lines

**Part 2:** Decode hexadecimal color codes where first 5 hex digits = distance, last digit = direction (0=R, 1=D, 2=L, 3=U)

Both parts use the same geometric approach, with Part 2 handling much larger distances (up to 1 million vs. single digits).

=== Performance Optimization

Day 18 was optimized by eliminating `strings.Fields` and pre-allocating slices with exact capacities.

==== Baseline (b0)

Initial implementation used string operations:
- `strings.Fields(line)` allocated slice of field strings for each line
- Dynamic vertex slice growth without pre-allocation
- Dynamic puzzle slice growth

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay18Part1-16               107.89µ ± 14%   132.03 KiB   685 allocs/op
BenchmarkDay18Part2-16               115.14µ ±  5%   132.03 KiB   685 allocs/op
----

==== Optimization (b1)

Three key changes:
- **Inline parsing**: Parse direction, distance, and color character-by-character without `strings.Fields`
- **Pre-allocate puzzle**: `make(Day18Puzzle, 0, len(lines))`
- **Pre-allocate vertices**: `make([][2]int, 0, len(puzzle)+1)` with exact capacity

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ /tmp/day18_b0_only.txt │       /tmp/day18_b1_only.txt        │
              │         sec/op         │   sec/op     vs base                │
Day18Part1-16            107.89µ ± 14%   17.26µ ± 4%  -84.00% (p=0.000 n=10)
Day18Part2-16            115.14µ ±  5%   18.91µ ± 3%  -83.58% (p=0.000 n=10)
geomean                   111.5µ         18.07µ       -83.79%

              │ /tmp/day18_b0_only.txt │        /tmp/day18_b1_only.txt        │
              │          B/op          │     B/op      vs base                │
Day18Part1-16            128.94Ki ± 0%   31.88Ki ± 0%  -75.28% (p=0.000 n=10)
Day18Part2-16            128.94Ki ± 0%   31.88Ki ± 0%  -75.28% (p=0.000 n=10)
geomean                   128.9Ki        31.88Ki       -75.28%

              │ /tmp/day18_b0_only.txt │       /tmp/day18_b1_only.txt       │
              │       allocs/op        │ allocs/op   vs base                │
Day18Part1-16             685.000 ± 0%   2.000 ± 0%  -99.71% (p=0.000 n=10)
Day18Part2-16             685.000 ± 0%   2.000 ± 0%  -99.71% (p=0.000 n=10)
geomean                     685.0        2.000       -99.71%
----

**Key Improvements:**

* **Part 1**: 84% faster (108µs → 17µs), 75% less memory (129KB → 32KB), 99.7% fewer allocations (685 → 2)
* **Part 2**: 84% faster (115µs → 19µs), 75% less memory (129KB → 32KB), 99.7% fewer allocations (685 → 2)

The optimization eliminates all string field splitting by parsing input character-by-character. The remaining 2 allocations are the puzzle slice and vertices slice, both pre-allocated with exact capacities. The 683 eliminated allocations (685→2) represent the per-line `strings.Fields` calls and dynamic slice growth operations.

== Day 19: Aplenty

=== Performance Optimization

Day 19 was optimized by eliminating `strings.Split` operations and pre-allocating data structures.

==== Baseline (b0)

Initial implementation used string operations heavily:
- `strings.IndexByte` for finding braces
- `strings.Split(rulesStr, ",")` for parsing rules
- `strings.Split(ruleStr, ":")` for conditional rules
- `strings.Split(line, ",")` and `strings.Split(attr, "=")` for parts parsing
- Dynamic map and slice growth without pre-allocation

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay19Part1-16               404.4µ ± 4%    308.2 KiB   4287 allocs/op
BenchmarkDay19Part2-16               427.7µ ± 3%    308.2 KiB   4287 allocs/op
----

==== Optimization (b1)

Four key changes:
- **Inline workflow parsing**: Parse rules character-by-character, find colons without `strings.Split`
- **Inline part parsing**: Parse attributes without `strings.Split`
- **Pre-allocate workflows map**: `make(map[string]workflow, 600)` with capacity hint
- **Pre-allocate parts slice**: `make([]part, 0, 200)`
- **Pre-allocate rule slices**: `make([]rule, 0, 4)` per workflow

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ /tmp/day19_b0_only.txt │      /tmp/day19_bench_b1.txt       │
              │         sec/op         │   sec/op     vs base               │
Day19Part1-16              404.4µ ± 4%   161.5µ ± 2%  -60.06% (p=0.000 n=10)
Day19Part2-16              427.7µ ± 3%   156.9µ ± 3%  -63.32% (p=0.000 n=10)
geomean                    415.9µ        159.2µ       -61.72%

              │ /tmp/day19_b0_only.txt │       /tmp/day19_bench_b1.txt        │
              │          B/op          │     B/op      vs base                │
Day19Part1-16             308.2Ki ± 0%   127.1Ki ± 0%  -58.76% (p=0.000 n=10)
Day19Part2-16             308.2Ki ± 0%   127.1Ki ± 0%  -58.76% (p=0.000 n=10)
geomean                   308.2Ki        127.1Ki       -58.76%

              │ /tmp/day19_b0_only.txt │      /tmp/day19_bench_b1.txt       │
              │       allocs/op        │ allocs/op   vs base                │
Day19Part1-16              4.287k ± 0%   0.586k ± 0%  -86.33% (p=0.000 n=10)
Day19Part2-16              4.287k ± 0%   0.586k ± 0%  -86.33% (p=0.000 n=10)
geomean                    4.287k        0.586k       -86.33%
----

**Key Improvements:**

* **Part 1**: 60% faster (404µs → 162µs), 59% less memory (308KB → 127KB), 86% fewer allocations (4287 → 586)
* **Part 2**: 63% faster (428µs → 157µs), 59% less memory (308KB → 127KB), 86% fewer allocations (4287 → 586)

The optimization eliminates all `strings.Split` operations by parsing character-by-character. Pre-allocating the workflows map (600 capacity), parts slice (200 capacity), and rule slices (4 capacity each) reduces growth operations. The 3701 eliminated allocations (4287→586) represent the string split operations and dynamic growth from the baseline.

== Day 20: Pulse Propagation

=== Performance Optimization

Day 20 was optimized by eliminating `strings.Split` operations and pre-allocating data structures.

==== Baseline (b0)

Initial implementation used string operations:
- `strings.Split(line, " -> ")` for parsing module and destinations
- `strings.Split(parts[1], ", ")` for parsing destination list
- Dynamic map and slice growth without pre-allocation
- Queue slicing with `queue = queue[1:]` creating allocations

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay20Part1-16               5.47ms ± 4%    5.43 MiB    9162 allocs/op
BenchmarkDay20Part2-16               23.1ms ± 4%    21.1 MiB   35864 allocs/op
----

==== Optimization (b1)

Four key changes:
- **Inline module parsing**: Find " -> " delimiter by checking 4-character sequence instead of `strings.Split`
- **Inline destination parsing**: Parse comma-separated destinations character-by-character
- **Pre-allocate structures**: `make(Day20Puzzle, len(lines))` for modules, `make(map[string]bool, 4)` for inputs
- **Pre-allocate queue**: `make([]pulse, 0, 100)` with capacity hint for BFS queue

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ /dev/fd/63  │             /dev/fd/62              │
              │   sec/op    │   sec/op     vs base                │
Day20Part1-16   5.466m ± 4%   3.009m ± 2%  -44.95% (p=0.000 n=10)
Day20Part2-16   23.06m ± 4%   12.97m ± 3%  -43.74% (p=0.000 n=10)
geomean         11.23m        6.247m       -44.35%

              │  /dev/fd/63   │              /dev/fd/62              │
              │     B/op      │     B/op      vs base                │
Day20Part1-16   5428.0Ki ± 0%   335.3Ki ± 0%  -93.82% (p=0.000 n=10)
Day20Part2-16   21.133Mi ± 0%   1.287Mi ± 0%  -93.91% (p=0.000 n=10)
geomean          10.58Mi        664.8Ki       -93.87%

              │  /dev/fd/63  │             /dev/fd/62              │
              │  allocs/op   │  allocs/op   vs base                │
Day20Part1-16    9162.0 ± 0%    427.0 ± 0%  -95.34% (p=0.000 n=10)
Day20Part2-16   35.864k ± 0%   1.144k ± 0%  -96.81% (p=0.000 n=10)
geomean          18.13k         698.9       -96.14%
----

**Key Improvements:**

* **Part 1**: 45% faster (5.47ms → 3.01ms), 94% less memory (5.43MB → 335KB), 95% fewer allocations (9162 → 427)
* **Part 2**: 44% faster (23.1ms → 13.0ms), 94% less memory (21.1MB → 1.29MB), 97% fewer allocations (35864 → 1144)

The optimization eliminates all `strings.Split` operations by finding delimiters through character-by-character inspection. Pre-allocating the puzzle map, inputs maps (4 capacity each), and BFS queue (100 capacity) reduces growth operations. The 8735 eliminated allocations in Part 1 (9162→427) represent the string split operations and queue slicing from the baseline.

== Day 21: Step Counter

=== Performance Optimization

Day 21 was optimized by pre-allocating data structures with capacity hints.

==== Baseline (b0)

Initial implementation used dynamic growth:
- Grid slice without pre-allocation
- `make(map[pos]int)` for visited tracking without capacity hint
- `make([]struct{...}, 0)` for queue without capacity hint
- Part 2 infinite grid uses larger structures but also grows dynamically

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay21Part1-16               2.16ms ± 3%    1.12 MiB     276 allocs/op
BenchmarkDay21Part2-16               111ms ± 7%     35.9 MiB    2556 allocs/op
----

==== Optimization (b1)

Three key changes:
- **Pre-allocate grid**: `make([][]byte, 0, len(lines))` with estimated capacity
- **Pre-allocate visited map**: `make(map[pos]int, height*width/2)` for Part 1, `make(map[pos]int, 50000)` for Part 2
- **Pre-allocate queue**: `make([]struct{...}, 0, 1000)` for Part 1, `make([]struct{...}, 0, 5000)` for Part 2

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ /dev/fd/63  │             /dev/fd/62              │
              │   sec/op    │   sec/op     vs base                │
Day21Part1-16   2.159m ± 3%   1.524m ± 5%  -29.41% (p=0.000 n=10)
Day21Part2-16   111.2m ± 7%   101.3m ± 6%   -8.91% (p=0.001 n=10)
geomean         15.50m        12.43m       -19.81%

              │  /dev/fd/63   │              /dev/fd/62              │
              │     B/op      │     B/op      vs base                │
Day21Part1-16   1122.5Ki ± 0%   734.2Ki ± 0%  -34.60% (p=0.000 n=10)
Day21Part2-16    35.91Mi ± 0%   33.13Mi ± 0%   -7.76% (p=0.000 n=10)
geomean          6.274Mi        4.873Mi       -22.33%

              │ /dev/fd/63  │             /dev/fd/62              │
              │  allocs/op  │  allocs/op   vs base                │
Day21Part1-16    276.0 ± 0%    203.0 ± 0%  -26.45% (p=0.000 n=10)
Day21Part2-16   2.556k ± 0%   2.153k ± 0%  -15.77% (p=0.000 n=10)
geomean          839.9         661.1       -21.29%
----

**Key Improvements:**

* **Part 1**: 29% faster (2.16ms → 1.52ms), 35% less memory (1.12MB → 734KB), 26% fewer allocations (276 → 203)
* **Part 2**: 9% faster (111ms → 101ms), 8% less memory (35.9MB → 33.1MB), 16% fewer allocations (2556 → 2153)

The optimization pre-allocates data structures with appropriate capacity hints based on the expected problem size. Part 1 benefits from estimating visited map size as half the grid (height×width/2) and queue size of 1000. Part 2 uses larger estimates (50000 for visited, 5000 for queue) for the infinite grid quadratic extrapolation. The remaining allocations primarily represent map bucket growth and queue slice operations that couldn't be eliminated without algorithmic changes.

== Day 22: Sand Slabs

=== Performance Optimization

Day 22 was optimized by pre-allocating data structures and replacing allocation-heavy operations with reusable arrays.

==== Baseline (b0)

Initial implementation used dynamic allocations:
- Bricks slice without pre-allocation
- `supports` and `supportedBy` slices without capacity hints
- Part 2 used `make(map[int]bool)` for fallen brick tracking (one per iteration)
- Part 2 used slice operations (`queue = queue[1:]`) causing repeated allocations

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay22Part1-16               20.4ms ± 1%     431 KiB    3140 allocs/op
BenchmarkDay22Part2-16               71.6ms ± 2%    5983 KiB    6295 allocs/op
----

==== Optimization (b2)

Four key changes:
- **Pre-allocate bricks**: `make(Day22Puzzle, 0, len(lines))` with estimated capacity
- **Pre-allocate support slices**: `make([]int, 0, 3)` for each brick's supports/supportedBy (average ~3 connections)
- **Reusable fallen array**: `make([]bool, len(bricks))` cleared and reused for all Part 2 iterations instead of map allocations
- **Array-based queue**: `make([]int, len(bricks))` with head/tail pointers instead of slice append/slicing operations

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ /tmp/day22_bench_b0.txt │       /tmp/day22_bench_b2.txt       │
              │         sec/op          │   sec/op     vs base                │
Day22Part1-16               20.40m ± 1%   20.03m ± 2%   -1.79% (p=0.009 n=10)
Day22Part2-16               71.55m ± 2%   21.94m ± 2%  -69.33% (p=0.000 n=10)
geomean                     38.20m        20.96m       -45.12%

              │ /tmp/day22_bench_b0.txt │       /tmp/day22_bench_b2.txt        │
              │          B/op           │     B/op      vs base                │
Day22Part1-16              430.6Ki ± 0%   292.7Ki ± 0%  -32.03% (p=0.000 n=10)
Day22Part2-16             5982.6Ki ± 0%   306.2Ki ± 0%  -94.88% (p=0.000 n=10)
geomean                    1.567Mi        299.4Ki       -81.35%

              │ /tmp/day22_bench_b0.txt │       /tmp/day22_bench_b2.txt       │
              │        allocs/op        │  allocs/op   vs base                │
Day22Part1-16               3.140k ± 0%   2.926k ± 0%   -6.82% (p=0.000 n=10)
Day22Part2-16               6.295k ± 0%   2.928k ± 0%  -53.49% (p=0.000 n=10)
geomean                     4.446k        2.927k       -34.16%
----

**Key Improvements:**

* **Part 1**: 2% faster (20.4ms → 20.0ms), 32% less memory (431KB → 293KB), 7% fewer allocations (3140 → 2926)
* **Part 2**: 69% faster (71.6ms → 21.9ms), 95% less memory (5983KB → 306KB), 53% fewer allocations (6295 → 2928)

The optimization dramatically improves Part 2 performance by eliminating repeated allocations in the BFS loop. The baseline created a new map for each of ~1400 brick removals tested; the optimized version reuses a single bool array, clearing it with a simple loop. The head/tail pointer queue approach eliminates slice reslicing operations (`queue = queue[1:]`) that create new slice headers. Pre-allocating support relationship slices with capacity 3 (typical brick connectivity) reduces growth operations. The remaining ~2926 allocations in both parts represent the initial brick parsing and support relationship building - unavoidable without changing the data model.

== Day 23: A Long Walk

=== Performance Optimization

Day 23 was optimized by pre-allocating data structures and using array-based queue with head/tail pointers.

==== Baseline (b0)

Initial implementation used dynamic allocations:
- Grid slice without pre-allocation
- `moves := [][2]int{}` created in Part 1 DFS hot loop without capacity
- Junction and graph maps without capacity hints
- `visited := make(map[pos]bool)` in Part 2 BFS loop without capacity
- Queue using slice operations (`queue = queue[1:]`) creating allocations

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay23Part1-16               12.7ms ± 5%     9.67 MiB   156505 allocs/op
BenchmarkDay23Part2-16               3.85s ± 1%      2.41 MiB    13470 allocs/op
----

==== Optimization (b2)

Six key changes:
- **Pre-allocate grid**: `make([][]byte, 0, len(lines))` with estimated capacity
- **Pre-allocate moves slice**: `make([][2]int, 0, 4)` in Part 1 DFS (max 4 directions)
- **Pre-allocate junctions map**: `make(map[pos]bool, 100)` with estimated capacity
- **Pre-allocate graph and edges**: `make(map[pos][]edge, len(junctions))` and `make([]edge, 0, 4)` per junction
- **Pre-allocate BFS structures**: `make(map[pos]bool, 1000)` for visited, `make([]struct{...}, 1, 1000)` for queue
- **Array-based queue with head/tail pointers**: Replace `queue = queue[1:]` with index-based traversal

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ /tmp/day23_bench_b0.txt │       /tmp/day23_bench_b2.txt       │
              │         sec/op          │   sec/op     vs base                │
Day23Part1-16              12.708m ± 5%   5.679m ± 2%  -55.31% (p=0.000 n=10)
Day23Part2-16                3.854 ± 1%    3.859 ± 2%        ~ (p=0.853 n=10)
geomean                     221.3m        148.0m       -33.11%

              │ /tmp/day23_bench_b0.txt │       /tmp/day23_bench_b2.txt        │
              │          B/op           │     B/op      vs base                │
Day23Part1-16            9672.93Ki ± 0%   46.41Ki ± 0%  -99.52% (p=0.000 n=10)
Day23Part2-16              2.413Mi ± 0%   1.908Mi ± 0%  -20.92% (p=0.000 n=10)
geomean                    4.774Mi        301.1Ki       -93.84%

              │ /tmp/day23_bench_b0.txt │      /tmp/day23_bench_b2.txt       │
              │        allocs/op        │ allocs/op   vs base                │
Day23Part1-16             156505.0 ± 0%   284.0 ± 0%  -99.82% (p=0.000 n=10)
Day23Part2-16              13470.0 ± 0%   367.0 ± 0%  -97.28% (p=0.000 n=10)
geomean                     45.91k        322.8       -99.30%
----

**Key Improvements:**

* **Part 1**: 55% faster (12.7ms → 5.68ms), 99.5% less memory (9.67MB → 46.4KB), 99.8% fewer allocations (156505 → 284)
* **Part 2**: Same speed (~0%), 21% less memory (2.41MB → 1.91MB), 97% fewer allocations (13470 → 367)

The optimization dramatically reduces Part 1's allocation overhead by pre-allocating the moves slice (capacity 4) in the DFS hot loop, eliminating 156k allocations. Part 2's BFS graph compression benefits from pre-allocated structures: junction map (100 capacity), graph map (sized to junctions), visited map (1000 capacity), and queue (1000 capacity). The head/tail pointer queue approach eliminates slice reslicing operations. Part 2's runtime remains unchanged because it's dominated by the final DFS on the compressed graph (which uses recursion), but memory usage and allocations improve significantly. The remaining 284 Part 1 allocations and 367 Part 2 allocations represent unavoidable parsing and graph construction overhead.

== Day 1: Trebuchet?!

Writing a custom one-pass parser for V2 because why not.

----
goos: darwin
goarch: amd64
pkg: adventofcode2023
cpu: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
BenchmarkDay01V1
BenchmarkDay01V1-16       	    5516	    214078 ns/op	    5006 B/op	    2207 allocs/op
BenchmarkDay01V2
BenchmarkDay01V2-16       	   21414	     55737 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay01Large
BenchmarkDay01Large-16    	     462	   2582112 ns/op	       0 B/op	       0 allocs/op
----
Solving part 1 takes 55 μs.

== Day 3: Gear Ratios

Looks like a flood fill, aka seed fill.
Doing my homework on the internet, lots of recursive solutions.
I personally avoid recursion, because it will blow up in your face,
right when the beeper goes on alert in the middle of deep sleep.

https://github.com/erich666/GraphicsGems/blob/master/gems/SeedFill.c[Heckbert]
has a solid looking implementation as part of his "Graphic Gems" Series in part 1, `A SEED FILL ALGORITHM`.
This one is also referenced by Wikipedia as `Span Filling`.

https://lodev.org/cgtutor/floodfill.html[Lode Vandevenne] has an interesting looking `Scanline Floodfill Algorithm With Stack (floodFillScanlineStack)`.

Then there's Windows C++ Code by https://www.codeproject.com/Articles/6017/QuickFill-An-efficient-flood-fill-algorithm[John R Shaw].

http://unity3dmc.blogspot.com/2017/02/ultimate-3d-floodfill-scanline.html[This] one claims to be 1000 times faster than anything else, but looks kinda weird.

https://en.wikipedia.org/wiki/Flood_fill#Walk-based_filling_(Fixed-memory_method)[Wikipedia] describes an O(1) memory `Walk-based filling (Fixed-memory method)`, but it is only valid for 4-connected, and we need 8-connected because diagonals count as well.

A version from 2007 named `A Linear-Time Constant-Space Algorithm for the Boundary Fill Problem` looks very efficient.
The pseudo-code provided is a real pseudo, as in

----
next.blockednodes:={u|dist(u,next)==1 && u.color!=black}
----

The newest publication i found is https://arxiv.org/abs/1906.03366[Scan-flood Fill(SCAFF): an Efficient Automatic Precise Region Filling Algorithm for Complicated Regions] by Yixuan He, Tianyi Hu, Delu Zeng.
It mentions large scale auto-learning.

But then we would have to XOR the whole thing because we need those numbers that are reachable, not islands.
Thinking again, KISS to the rescue. Just parse numbers, and check if the C8 environment contains any special characters.

It took me three turns to make the algorithm work. Yes, numbers can end directly at the end of the line.

image::static/day03.png[]

=== Power Mode 'Balanced'

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 7 7840HS w/ Radeon 780M Graphics
BenchmarkDay03Part1-16    	   19702	     57430 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part1-16    	   41658	     32092 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part1-16    	   41925	     28282 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part1-16    	   42114	     29990 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part1-16    	   38536	     28438 ns/op	       0 B/op	       0 allocs/op
----

=== Power Mode 'Power Saver'

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 7 7840HS w/ Radeon 780M Graphics
BenchmarkDay03Part1-16    	   10586	    111056 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part1-16    	   10000	    101209 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part1-16    	   10000	    112834 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part1-16    	   10000	    115926 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part1-16    	   10000	    101594 ns/op	       0 B/op	       0 allocs/op
----

=== Power Mode 'Performance'

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 7 7840HS w/ Radeon 780M Graphics
BenchmarkDay03Part1-16    	   42468	     30711 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part1-16    	   38066	     30958 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part1-16    	   42180	     28016 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part1-16    	   37698	     31678 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part1-16    	   41976	     30219 ns/op	       0 B/op	       0 allocs/op
----

=== Part 2

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 7 7840HS w/ Radeon 780M Graphics
BenchmarkDay03Part2-16             74426             15353 ns/op               0 B/op          0 allocs/op
BenchmarkDay03Part2-16             78756             15988 ns/op               0 B/op          0 allocs/op
BenchmarkDay03Part2-16             77714             15708 ns/op               0 B/op          0 allocs/op
BenchmarkDay03Part2-16             72922             15024 ns/op               0 B/op          0 allocs/op
BenchmarkDay03Part2-16             77499             15244 ns/op               0 B/op          0 allocs/op
BenchmarkDay03Part2-16             74164             16153 ns/op               0 B/op          0 allocs/op
BenchmarkDay03Part2-16             74095             15557 ns/op               0 B/op          0 allocs/op
BenchmarkDay03Part2-16             77061             15753 ns/op               0 B/op          0 allocs/op
BenchmarkDay03Part2-16             74888             15874 ns/op               0 B/op          0 allocs/op
BenchmarkDay03Part2-16             71414             16128 ns/op               0 B/op          0 allocs/op
----

----
goos: darwin
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Core(TM) i9-9980HK CPU @ 2.40GHz
BenchmarkDay03Part2-16    	   34120	     35665 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-16    	   35691	     33231 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-16    	   36195	     33311 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-16    	   35734	     35594 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-16    	   35541	     33427 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-16    	   35611	     33519 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-16    	   35823	     34799 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-16    	   33032	     33603 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-16    	   36066	     33785 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-16    	   35368	     33881 ns/op	       0 B/op	       0 allocs/op
----

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 5 3400G with Radeon Vega Graphics
BenchmarkDay03Part2-8   	   29594	     39583 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-8   	   30244	     39643 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-8   	   31123	     40310 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-8   	   27504	     38278 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-8   	   26856	     40871 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-8   	   28608	     39918 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-8   	   28083	     40723 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-8   	   28372	     40097 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-8   	   27661	     40784 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-8   	   30015	     41826 ns/op	       0 B/op	       0 allocs/op
----

== Day 4: Scratchcards

=== Performance Optimization

Day 04 Part 2 was optimized to achieve zero allocations by replacing a dynamic slice with a fixed-size array.

==== Baseline (b0)

Initial implementation used dynamic slice allocation for tracking card counts:
- `ns := make([]uint, cardCount)` allocated a slice based on actual card count
- For 224 cards, this resulted in 1792 bytes (224 × 8 bytes/uint)
- Part 2 had 1 allocation per call

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay04Part1-16              36.28µ ± 2%
BenchmarkDay04Part2-16              53.40µ ± 1%    1792 B/op    1 allocs/op
----

==== Optimization (b1)

Replaced dynamic slice with fixed-size stack-allocated array:
- `var ns [256]uint` uses compile-time known size
- Array allocated on stack instead of heap
- Comment documents safety: "AoC inputs have <256 cards, so this is safe"
- Loop over actual card count using `for i := range cardCount`

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ /tmp/day04_bench_b0.txt │       /tmp/day04_bench_b1.txt       │
              │         sec/op          │   sec/op     vs base                │
Day04Part1-16               36.28µ ± 2%   43.86µ ± 2%  +20.90% (p=0.000 n=10)
Day04Part2-16               53.40µ ± 1%   52.28µ ± 1%   -2.08% (p=0.001 n=10)
geomean                     44.01µ        47.89µ        +8.80%

              │ /tmp/day04_bench_b0.txt │          /tmp/day04_bench_b1.txt          │
              │          B/op           │     B/op      vs base                     │
Day04Part1-16              0.000 ± 0%       0.000 ± 0%         ~ (p=1.000 n=10) ¹
Day04Part2-16            1.750Ki ± 0%     0.000Ki ± 0%  -100.00% (p=0.000 n=10)
geomean                               ²                 ?                       ² ³
¹ all samples are equal
² summaries must be >0 to compute geomean
³ ratios must be >0 to compute geomean

              │ /tmp/day04_bench_b0.txt │         /tmp/day04_bench_b1.txt         │
              │        allocs/op        │ allocs/op   vs base                     │
Day04Part1-16              0.000 ± 0%     0.000 ± 0%         ~ (p=1.000 n=10) ¹
Day04Part2-16              1.000 ± 0%     0.000 ± 0%  -100.00% (p=0.000 n=10)
geomean                               ²               ?                       ² ³
¹ all samples are equal
² summaries must be >0 to compute geomean
³ ratios must be >0 to compute geomean
----

**Key Improvements:**

* **Part 2**: 2.08% faster (53.4µs → 52.3µs)
* **Part 2**: 100% reduction in allocations (1 → 0 allocs/op)
* **Part 2**: 100% reduction in memory (1792 B → 0 B)

The optimization achieves zero-allocation performance by leveraging compile-time knowledge about input constraints. AoC puzzle inputs have fewer than 256 cards, making a fixed-size `[256]uint` array safe. The Go compiler allocates this array on the stack rather than the heap, eliminating the dynamic allocation overhead entirely. The array uses 2048 bytes of stack space (256 × 8), but only the first `cardCount` elements are accessed.

== Day 5: If You Give A Seed A Fertilizer

=== Performance Optimization

Day 05 was optimized by pre-allocating slices in parsing to reduce memory allocations.

==== Baseline (b0)

Initial implementation used dynamic slice growth with `append()` operations without capacity hints:
- `parseDay05` allocated slices without initial capacity
- Resulted in multiple reallocations as slices grew

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay05Part1-16    	   18293	     64980 ns/op	   40960 B/op	     972 allocs/op
BenchmarkDay05Part2-16    	    3074	    390274 ns/op	  253200 B/op	    8322 allocs/op
----

==== Optimization (b1)

Pre-allocated slices with estimated capacities:
- `parseDay05`: Pre-allocate `rss` with capacity 7 (typical number of map sections)
- `parseDay05`: Pre-allocate `rs` with capacity 40 per map section
- `Day05`: Pre-allocate `seedRanges` with exact capacity `len(seeds)/2`

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ benches/day05_bench_b0.txt │   benches/day05_bench_b1_v2.txt    │
              │           sec/op           │   sec/op     vs base               │
Day05Part1-16                  64.98µ ± 2%   64.01µ ± 3%  -1.49% (p=0.035 n=10)
Day05Part2-16                  390.3µ ± 3%   377.6µ ± 2%  -3.25% (p=0.011 n=10)
geomean                        159.2µ        155.5µ       -2.37%

              │ benches/day05_bench_b0.txt │    benches/day05_bench_b1_v2.txt    │
              │            B/op            │     B/op      vs base               │
Day05Part1-16                 40.00Ki ± 0%   37.23Ki ± 0%  -6.91% (p=0.000 n=10)
Day05Part2-16                 247.3Ki ± 0%   244.0Ki ± 0%  -1.32% (p=0.000 n=10)
geomean                       99.45Ki        95.32Ki       -4.16%

              │ benches/day05_bench_b0.txt │   benches/day05_bench_b1_v2.txt    │
              │         allocs/op          │  allocs/op   vs base               │
Day05Part1-16                   972.0 ± 0%    934.0 ± 0%  -3.91% (p=0.000 n=10)
Day05Part2-16                  8.322k ± 0%   8.280k ± 0%  -0.50% (p=0.000 n=10)
geomean                        2.844k        2.781k       -2.22%
----

**Key Improvements:**

* **Part 1**: 1.49% faster (65.0µs → 64.0µs), 6.91% less memory (40KB → 37KB), 3.91% fewer allocations (972 → 934)
* **Part 2**: 3.25% faster (390µs → 378µs), 1.32% less memory (247KB → 244KB), 0.50% fewer allocations (8322 → 8280)

The optimization works by eliminating repeated slice reallocations during parsing. By pre-allocating slices with appropriate capacities based on known input patterns, the Go runtime avoids multiple copy operations as slices grow. The parsing phase benefits most from this optimization, as it processes hundreds of numbers across multiple map sections.

=== Part 2

Part 2 reinterprets the seed numbers as ranges (start, length) instead of individual seeds.
This makes brute force iteration infeasible - the example has 27 seeds, but the real input has billions.

The solution uses range splitting to process entire ranges through each map layer without enumerating individual values. <1>
When a range overlaps with a map entry, it splits into:

- Unmapped portions (before and after the overlap) that pass through unchanged
- Overlapped portion that transforms to destination space

Each range tracks its Min/Max values in the current space (seed → soil → ... → location).

----
Answer: 52210644
----
<1> First attempt accumulated deltas (offset transformations) but kept ranges in seed space.
This produced answer 14209264, which was too low because the final minimum was calculated in the wrong coordinate system.

== Day 6: Wait For It

=== Performance Optimization

Day 06 was optimized by replacing brute force iteration with a mathematical solution using the quadratic formula.

==== Baseline (b0)

Initial implementation iterated through all possible hold times to count winning strategies:
- `Farther()` used a loop from 1 to t-1, testing each value
- For Part 1 (small races), this was fast
- For Part 2 (single race with t=42,686,985), this meant ~42 million iterations

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay06Part1-16              212.25n ± 3%
BenchmarkDay06Part2-16         35521415.50n ± 3%    (35.5 milliseconds)
----

==== Optimization (b1)

The problem is actually a quadratic equation:
- We want: `h*(t-h) > d` where h is hold time, t is total time, d is record distance
- Rearranging: `h² - h*t + d < 0`
- Use quadratic formula to find boundary points: `h = (t ± sqrt(t² - 4d)) / 2`
- Count integers strictly between the two roots

This changes the algorithm from O(n) iteration to O(1) calculation.

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ /tmp/day06_bench_b0.txt │       /tmp/day06_bench_b1.txt        │
              │         sec/op          │   sec/op     vs base                 │
Day06Part1-16              212.25n ± 3%   41.81n ± 3%   -80.30% (p=0.000 n=10)
Day06Part2-16         35521415.50n ± 3%   11.01n ± 3%  -100.00% (p=0.000 n=10)
geomean                     86.83µ        21.46n        -99.98%
----

**Key Improvements:**

* **Part 1**: 80.3% faster (212ns → 42ns) - 5x speedup
* **Part 2**: 99.99997% faster (35.5ms → 11ns) - **3.2 million times faster!**

The optimization transforms the problem from iterating through millions of values to a single mathematical calculation. The quadratic formula gives exact boundary points, eliminating the need to test each individual hold time. Part 2's dramatic improvement (from 35 milliseconds to 11 nanoseconds) demonstrates the power of choosing the right algorithm - O(1) vs O(n) where n=42 million.

== Day 7: Camel Cards

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 7 7840HS w/ Radeon 780M Graphics
BenchmarkDay07Part1-16    	     494	   2360739 ns/op
----

For part 2, i was promoting `OnePair` and a Joker to `TwoPairs` instead of `ThreeOfAKind`. Damn.

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 5 3400G with Radeon Vega Graphics
BenchmarkDay07Part1-8   	     220	   4891596 ns/op	   72576 B/op	    1001 allocs/op
BenchmarkDay07Part2-8   	     222	   5629774 ns/op	   72576 B/op	    1001 allocs/op
----

To get rid of the memory allocation, pack cards and their bid into a struct, so it is not required to backreference the bid after ranking.

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 5 3400G with Radeon Vega Graphics
BenchmarkDay07Part1-8   	    5211	    222719 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay07Part2-8   	    4837	    233281 ns/op	       0 B/op	       0 allocs/op
----

Thats 230 μs, and no memory allocation.

== Haunted Wasteland

A straight forward puzzle to implement, no algorithm background required.

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 5 3400G with Radeon Vega Graphics
BenchmarkDay08Part1-8   	    1884	    554004 ns/op	       0 B/op	       0 allocs/op
----

=== Part 2

Part 2 requires starting at all nodes ending with 'A' simultaneously and moving all "ghosts" in sync until ALL reach nodes ending with 'Z' at the same time.

The naive approach of simulating all ghosts step-by-step works for the example (6 steps) but would never terminate for the actual input - it would need to run for over 18 trillion steps. <1>

The solution exploits a special property of the input: each ghost's path forms a cycle with exactly one Z node at regular intervals. By finding when each ghost first reaches a Z node, we can use the Least Common Multiple (LCM) to calculate when all cycles align.

For each starting node ending in 'A':
- Follow the path until reaching a node ending in 'Z'
- Record the number of steps (cycle length)
- Calculate LCM of all cycle lengths

----
Answer: 18024643846273
----
<1> First attempt used synchronous simulation which ran >10s without completing, making it clear the input requires mathematical insight rather than brute force.

=== Performance Optimization

Day 08 was optimized by pre-allocating slices with exact capacities to reduce memory allocations.

==== Baseline (b0)

Initial implementation used dynamic slice growth without capacity hints:
- `var cycleLengths []uint` in Part 2 grew dynamically via append
- `var nodes []string` in `startNodes()` grew dynamically via append
- Part 2 resulted in 12 allocations per call

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay08Part1-16              615.4µ ± 1%     56.09 KiB    4 allocs/op
BenchmarkDay08Part2-16              3.226m ± 2%     56.44 KiB   12 allocs/op
----

==== Optimization (b1)

Pre-allocated slices with exact capacities:
- `cycleLengths := make([]uint, 0, len(starts))` - pre-allocate for known number of starting nodes
- In `startNodes()`: count nodes first, then pre-allocate with exact capacity
- Eliminates repeated reallocation as slices grow

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ /tmp/day08_bench_b0.txt │      /tmp/day08_bench_b1.txt       │
              │         sec/op          │   sec/op     vs base               │
Day08Part1-16               615.4µ ± 1%   557.2µ ± 1%  -9.47% (p=0.000 n=10)
Day08Part2-16               3.226m ± 2%   3.125m ± 1%  -3.11% (p=0.000 n=10)
geomean                     1.409m        1.320m       -6.34%

              │ /tmp/day08_bench_b0.txt │       /tmp/day08_bench_b1.txt       │
              │          B/op           │     B/op      vs base               │
Day08Part1-16              56.09Ki ± 0%   56.09Ki ± 0%       ~ (p=0.582 n=10)
Day08Part2-16              56.44Ki ± 0%   56.23Ki ± 0%  -0.37% (p=0.000 n=10)
geomean                    56.26Ki        56.16Ki       -0.19%

              │ /tmp/day08_bench_b0.txt │       /tmp/day08_bench_b1.txt        │
              │        allocs/op        │ allocs/op   vs base                  │
Day08Part1-16                4.000 ± 0%   4.000 ± 0%        ~ (p=1.000 n=10) ¹
Day08Part2-16               12.000 ± 0%   6.000 ± 0%  -50.00% (p=0.000 n=10)
geomean                      6.928        4.899       -29.29%
¹ all samples are equal
----

**Key Improvements:**

* **Part 1**: 9.47% faster (615µs → 557µs)
* **Part 2**: 3.11% faster (3.23ms → 3.13ms), 50% reduction in allocations (12 → 6 allocs/op)

The optimization reduces allocations by eliminating dynamic slice growth. Pre-allocating with exact capacity prevents the Go runtime from repeatedly allocating larger backing arrays and copying data as slices grow. Part 2 benefits most because it processes multiple starting nodes, each requiring cycle length tracking. The `startNodes()` optimization uses a two-pass approach: count first, then allocate exact capacity - trading a small amount of iteration overhead for predictable memory allocation.

== Day 9: Mirage Maintenance

These timings include parsing the puzzle input:

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 5 3400G with Radeon Vega Graphics
BenchmarkDay09Part1-8   	    5962	    378392 ns/op	   70400 B/op	     200 allocs/op
----

The corresponding CPU profile shows that most of the time is spent parsing.

image::static/profile09-01.svg[Embedded,800,opts=inline]

Spinning a custom parser:

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 5 3400G with Radeon Vega Graphics
BenchmarkDay09V1Part1-8   	    6135	    398102 ns/op	   70400 B/op	     200 allocs/op
BenchmarkDay09V2Part1-8   	    9898	    113864 ns/op	       0 B/op	       0 allocs/op
----

400% faster, and no more memory allocations.

image::static/profile09-02.svg[Embedded,800,opts=inline]

Function names from the parser's state machine are mangled, such as `func1`.

----
ROUTINE ======================== gitlab.com/jhinrichsen/adventofcode2023.Day09V2.func1 in /home/jot/repos/adventofcode2023/day09.go <1>
     430ms      430ms (flat, cum) 19.46% of Total
         .          .     74:	clear := func() { <2>
         .          .     75:		for y := range ns {
     430ms      430ms     76:			for x := range ns[y] {
         .          .     77:				ns[y][x] = -1
         .          .     78:			}
         .          .     79:		}
         .          .     80:	}
         .          .     81:	clear()
----
<1> mangled function name
<2> function name

The `clear()/ func1` function helped to make sure all (x/y) positions are correct, it can safely be removed now.

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 5 3400G with Radeon Vega Graphics
BenchmarkDay09V2Part1-8   	   14719	     83811 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09V2Part1-8   	   13400	     84512 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09V2Part1-8   	   13508	     84898 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09V2Part1-8   	   14628	     84776 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09V2Part1-8   	   14185	     84671 ns/op	       0 B/op	       0 allocs/op
----

As suggested, removing `clear()/ func1` shaves off another 20% from 114 to 84 μs.
20% is spent in an inlined function building numbers from digits:

----
ROUTINE ======================== gitlab.com/jhinrichsen/adventofcode2023.Day09V2.func2 in /home/jot/repos/adventofcode2023/day09.go
     1.84s      1.84s (flat, cum) 19.70% of Total
         .          .     79:	digit := func(d int) {
     1.84s      1.84s     80:		n = 10*n + d
         .          .     81:	}
----

ChatGPT 4o confirms that this is already pretty optimized, and slightly suggests trying bit shifting `(n << 3) + (n << 1) + d`.
The Go compiler decides to take another route, the disassembly for `n = 10 * n + d` on AMD looks like this:

----
     170ms      170ms     50f6ca: MOVQ 0x4e220(SP), DI                    ;gitlab.com/jhinrichsen/adventofcode2023.Day09V2 day09.go:80
     980ms      980ms     50f6d2: LEAQ 0(DI)(DI*4), DI
      90ms       90ms     50f6d6: LEAQ 0(SI)(DI*2), SI
     600ms      600ms     50f6da: MOVQ SI, 0x4e220(SP)
----

`DI + DI * 4` (5) and then `SI + DI * 2` (10 * DI + SI).
Well, compiler people know way better than me, so i leave it like that.

For part 2, i had to add a single line, and BÄM

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 5 3400G with Radeon Vega Graphics
BenchmarkDay09Part2-8   	   13662	     86871 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09Part2-8   	   12366	     86868 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09Part2-8   	   13159	     88358 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09Part2-8   	   13674	     89267 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09Part2-8   	   12960	     89872 ns/op	       0 B/op	       0 allocs/op
----

This one line for part 2 adds 3% runtime penalty.

----
name          old time/op    new time/op    delta
Day09Part2-8    87.2µs ± 2%    89.4µs ± 3%   ~     (p=0.095 n=5+5)

name          old alloc/op   new alloc/op   delta
Day09Part2-8     0.00B          0.00B        ~     (all equal)

name          old allocs/op  new allocs/op  delta
Day09Part2-8      0.00           0.00        ~     (all equal)
----

Framework 16" in Performance Mode:

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 7 7840HS w/ Radeon 780M Graphics
BenchmarkDay09V1Part1-16    	   13080	     91186 ns/op	   70400 B/op	     200 allocs/op
BenchmarkDay09V1Part1-16    	   13360	     90379 ns/op	   70400 B/op	     200 allocs/op
BenchmarkDay09V1Part1-16    	   13119	     91153 ns/op	   70400 B/op	     200 allocs/op
BenchmarkDay09V1Part1-16    	   13088	     91441 ns/op	   70400 B/op	     200 allocs/op
BenchmarkDay09V1Part1-16    	   13124	     91989 ns/op	   70400 B/op	     200 allocs/op
BenchmarkDay09V2Part1-16    	   28809	     41317 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09V2Part1-16    	   29056	     41194 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09V2Part1-16    	   28704	     41308 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09V2Part1-16    	   28743	     41222 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09V2Part1-16    	   29160	     41331 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09Part2-16      	   27960	     42377 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09Part2-16      	   28038	     42533 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09Part2-16      	   28312	     42262 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09Part2-16      	   28311	     42283 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09Part2-16      	   28095	     42474 ns/op	       0 B/op	       0 allocs/op
----

and in Power Saver Mode:

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 7 7840HS w/ Radeon 780M Graphics
BenchmarkDay09V1Part1-16    	    3446	    298572 ns/op	   70400 B/op	     200 allocs/op
BenchmarkDay09V1Part1-16    	    3676	    296005 ns/op	   70400 B/op	     200 allocs/op
BenchmarkDay09V1Part1-16    	    3806	    294459 ns/op	   70400 B/op	     200 allocs/op
BenchmarkDay09V1Part1-16    	    3470	    297215 ns/op	   70400 B/op	     200 allocs/op
BenchmarkDay09V1Part1-16    	    3666	    297516 ns/op	   70400 B/op	     200 allocs/op

BenchmarkDay09V2Part1-16    	    7330	    143001 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09V2Part1-16    	    7080	    148411 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09V2Part1-16    	    8065	    135634 ns/op	       0 B/op	       0 allocs/op <1>
BenchmarkDay09V2Part1-16    	   28040	     40523 ns/op	       0 B/op	       0 allocs/op <2>
BenchmarkDay09V2Part1-16    	   29444	     40772 ns/op	       0 B/op	       0 allocs/op

BenchmarkDay09Part2-16      	   28267	     42077 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09Part2-16      	   28153	     42776 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09Part2-16      	   27757	     44260 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09Part2-16      	   28194	     41942 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay09Part2-16      	   28086	     42086 ns/op	       0 B/op	       0 allocs/op
----
<1> right before turbo
<2> in turbo

Interesting that power mode starts about 3 x slower, but then reaches the same performance for part 2.
Looks like power saver mode is more resistent to CPU peaks, but then goes all in when required.

=== Performance Optimization (Stack Allocation)

Day 09 was further optimized by reducing array dimensions to fit within stack size limits, eliminating heap escaping.

==== Baseline (b0)

The Day09V2 implementation used a fixed-size array `var ns [DIM][DIM]int` with `DIM = 200`:
- Array size: [200][200]int = 200 × 200 × 8 bytes = 320 KB
- Exceeded typical stack frame limits, causing heap allocation
- Resulted in 1 allocation per call despite using a "fixed-size" array

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay09Part1-16             171.18µ ± 27%    320 KiB    1 allocs/op
BenchmarkDay09Part2-16             206.22µ ±  6%    320 KiB    1 allocs/op
----

==== Optimization (b1)

Analyzed actual input dimensions:
- 200 lines (matching original DIM)
- Maximum 21 numbers per line (much less than 200)

Reduced array to actual requirements with safety margin:
- Changed from `var ns [200][200]int` to `var ns [LINES][MAXNUMS]int`
- Where `LINES = 200` and `MAXNUMS = 30`
- New array size: [200][30]int = 200 × 30 × 8 bytes = 48 KB
- Now fits comfortably within stack limits

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ /tmp/day09_bench_b0.txt │       /tmp/day09_bench_b1.txt       │
              │         sec/op          │   sec/op     vs base                │
Day09Part1-16             171.18µ ± 27%   75.61µ ± 2%  -55.83% (p=0.000 n=10)
Day09Part2-16             206.22µ ±  6%   77.02µ ± 2%  -62.65% (p=0.000 n=10)
geomean                    187.9µ         76.31µ       -59.38%

              │ /tmp/day09_bench_b0.txt │         /tmp/day09_bench_b1.txt         │
              │          B/op           │    B/op     vs base                     │
Day09Part1-16              320.0Ki ± 0%   0.0Ki ± 0%  -100.00% (p=0.000 n=10)
Day09Part2-16              320.0Ki ± 0%   0.0Ki ± 0%  -100.00% (p=0.000 n=10)
geomean                    320.0Ki                    ?                       ¹ ²
¹ summaries must be >0 to compute geomean
² ratios must be >0 to compute geomean

              │ /tmp/day09_bench_b0.txt │         /tmp/day09_bench_b1.txt         │
              │        allocs/op        │ allocs/op   vs base                     │
Day09Part1-16                1.000 ± 0%   0.000 ± 0%  -100.00% (p=0.000 n=10)
Day09Part2-16                1.000 ± 0%   0.000 ± 0%  -100.00% (p=0.000 n=10)
geomean                      1.000                    ?                       ¹ ²
¹ summaries must be >0 to compute geomean
² ratios must be >0 to compute geomean
----

**Key Improvements:**

* **Part 1**: 55.83% faster (171µs → 76µs), 100% reduction in allocations and memory (320 KB → 0 B)
* **Part 2**: 62.65% faster (206µs → 77µs), 100% reduction in allocations and memory (320 KB → 0 B)

The optimization eliminates heap escaping by sizing arrays based on actual input dimensions rather than worst-case estimates. Go's compiler allocates small arrays (<64KB typically) on the stack, but large arrays escape to the heap. By reducing from 320KB to 48KB, the array now stays on the stack. This eliminates allocation overhead and improves cache locality. The analysis step (checking input dimensions) is crucial - blindly using large fixed arrays can defeat the performance benefits of avoiding dynamic allocations.

== Day 10: Pipe Maze

I prompted ChatGPT for the first example puzzle, taking around 10 iterations aggregating details.
Right down to the location of the input puzzle file, and its filename.
Plus a unit test that uses the example puzzle input.
The resulting code is pretty standard stuff, at least junior Go developer.

But then, running the tests `panic()`s.

----
$ go test -run=Day10
--- FAIL: TestDay10Part1ExampleChatGPT (0.00s)
panic: runtime error: index out of range [0] with length 0 [recovered]
	panic: runtime error: index out of range [0] with length 0
----

Seems as if some structure is not properly initialized.
`LoadGridFromFile` contains solid line parser code with a twist.

----
        var grid *Day10Grid <1>
        scanner := bufio.NewScanner(file)
        y := 0
        for scanner.Scan() {
                line := scanner.Text()
                if grid == nil { <1>
                        grid = Day10NewGrid(len(line), 0) <2>
                }
                grid.height++ <3>
                for x, char := range line {
                        grid.SetExits(x, y, string(char)) <4>
                }
                y++
        }

        return grid, scanner.Err()
----
<1> declaration for lazy initialization of a 2D grid until width is known
<2> initializes correct width, but height 0
<3> increasing the height afterwards, this will work because underlying type is an int
<4> trying to store exit of cells, but no cells inside the grid have been allocated

Manually fixing the incorrect memory management, we get a test result.
It is not the expected 4 steps, but `with a distance of 2 steps`.

ChatGPT applies a standard BFS, and instead of counting the steps as in

----
-L|F7        .....
7S-7|        .S12.
L|7||        .1.3.
-L-J|        .234.
L|-JF        .....
----

it tracks the distance to S.

----
-L|F7        .....
7S-7|        .S...
L|7||        ..1..
-L-J|        ...2.
L|-JF        .....
----

I am trying different prompts, but cannot get ChatGPT off the BFS.
Gemini to the rescue? Not really.
The code looks a little bit more friendly, but this is just a matter of taste i guess.
Nevertheless, the result is also wrong.
Instead of 4, ChatGPT calculates 2, Gemini 6.

Rolling my own version...

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 5 3400G with Radeon Vega Graphics
BenchmarkDay10Part1-8   	   11338	    106754 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay10Part1-8   	   11493	    107613 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay10Part1-8   	   11535	    101549 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay10Part1-8   	   10000	    105559 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay10Part1-8   	   10000	    103705 ns/op	       0 B/op	       0 allocs/op
----

Benchmarking different implementation of `opposite()` direction:

----
 19 func opposite1(d direction) direction {
 20         switch d {
 21         case North:
 22                 return South
 23         case South:
 24                 return North
 25         case West:
 26                 return East
 27         case East:
 28                 return West
 29         }
 30         return d
 31 }
 32

TEXT command-line-arguments.opposite1(SB) /home/jot/repos/adventofcode2023/day10.go
  day10.go:20		0x61ac			3c02			CMPL AL, $0x2
  day10.go:23		0x61ae			7714			JA 0x61c4
  day10.go:21		0x61b0			3c01			CMPL AL, $0x1
  day10.go:21		0x61b2			740a			JE 0x61be
  day10.go:20		0x61b4			3c02			CMPL AL, $0x2
  day10.go:23		0x61b6			7518			JNE 0x61d0
  day10.go:24		0x61b8			b801000000		MOVL $0x1, AX
  day10.go:24		0x61bd			c3			RET
  day10.go:22		0x61be			b802000000		MOVL $0x2, AX
  day10.go:22		0x61c3			c3			RET
  day10.go:25		0x61c4			3c04			CMPL AL, $0x4
  day10.go:25		0x61c6			740f			JE 0x61d7
  day10.go:25		0x61c8			0f1f4000		NOPL 0(AX)
  day10.go:27		0x61cc			3c08			CMPL AL, $0x8
  day10.go:27		0x61ce			7401			JE 0x61d1
  day10.go:30		0x61d0			c3			RET
  day10.go:28		0x61d1			b804000000		MOVL $0x4, AX
  day10.go:28		0x61d6			c3			RET
  day10.go:26		0x61d7			b808000000		MOVL $0x8, AX
  day10.go:26		0x61dc			c3			RET
----

----
 33 func opposite2(d direction) direction {
 34         switch d {
 35         case North:
 36                 d = South
 37         case South:
 38                 d = North
 39         case West:
 40                 d = East
 41         case East:
 42                 d = West
 43         }
 44         return d
 45 }

TEXT command-line-arguments.opposite2(SB) /home/jot/repos/adventofcode2023/day10.go
  day10.go:34		0x61dd			3c02			CMPL AL, $0x2
  day10.go:37		0x61df			7716			JA 0x61f7
  day10.go:35		0x61e1			3c01			CMPL AL, $0x1
  day10.go:35		0x61e3			7507			JNE 0x61ec
  day10.go:35		0x61e5			b802000000		MOVL $0x2, AX
  day10.go:36		0x61ea			eb1f			JMP 0x620b
  day10.go:34		0x61ec			3c02			CMPL AL, $0x2
  day10.go:37		0x61ee			751b			JNE 0x620b
  day10.go:37		0x61f0			b801000000		MOVL $0x1, AX
  day10.go:38		0x61f5			eb14			JMP 0x620b
  day10.go:39		0x61f7			3c04			CMPL AL, $0x4
  day10.go:39		0x61f9			7507			JNE 0x6202
  day10.go:39		0x61fb			b808000000		MOVL $0x8, AX
  day10.go:40		0x6200			eb09			JMP 0x620b
  day10.go:41		0x6202			3c08			CMPL AL, $0x8
  day10.go:41		0x6204			7505			JNE 0x620b
  day10.go:41		0x6206			b804000000		MOVL $0x4, AX
  day10.go:44		0x620b			c3			RET
----

Comparing `opposite1()` against `opposite2()` reveals no performance impact.

----
$ benchstat opposite1.bench opposite2.bench
name        old time/op    new time/op    delta
Opposite-8    1.58µs ± 4%    1.58µs ± 7%   ~     (p=0.591 n=10+10)
----

For the next implementation, we calculate the bit position, and shift between corresponding pairs (N <-> S, W <-> E).

----
 47 func opposite3(in direction) direction {
 48         var out direction
 49         bitPosition := bits.Len8(uint8(in))
 50         if bitPosition%2 == 0 {
 51                 out = in >> 1
 52         } else {
 53                 out = in << 1
 54         }
 55         return out
 56 }

TEXT command-line-arguments.opposite3(SB) /home/jot/repos/adventofcode2023/day10.go
  day10.go:49		0x620c			0fb6c8			MOVZX AL, CX
  day10.go:49		0x620f			8d0c09			LEAL 0(CX)(CX*1), CX
  day10.go:49		0x6212			8d4901			LEAL 0x1(CX), CX
  day10.go:49		0x6215			0fbdc9			BSRL CX, CX <1>
  day10.go:50		0x6218			0fbae100		BTL $0x0, CX <2>
  day10.go:50		0x621c			7204			JB 0x6222
  day10.go:51		0x621e			d0e8			SHRL $0x1, AL
  day10.go:51		0x6220			eb02			JMP 0x6224
  day10.go:53		0x6222			d1e0			SHLL $0x1, AX
  day10.go:55		0x6224			c3			RET
----
<1> Bit Scan Reverse
<2> Bit Test, n % 2 == 0

Now this one flies.

----
$ benchstat opposite1.bench opposite3.bench
name        old time/op    new time/op    delta
Opposite-8    1.58µs ± 4%    0.31µs ± 3%  -80.70%  (p=0.000 n=10+9)
----

Although, to be honest, there's not much difference in total runtime because it is called exactly once for each step through the maze,
which gives it O(n) complexity.

=== Part 2

For part 2, the way through the maze needs to be marked so that a floodfill algorithm can be used.
Instead of copying the maze into another data structure, i want to keep track of touched cells while traversing.
The puzzle input is passed as []string, which are immutable in Go, so input type is changed from `[]string` to `[][]byte`.
While we're at it, why not memory map the puzzle input using `syscall.Mmap()`.
Then, in one sweep, use `unsafe.Slice()` to have `[]byte` pointers into the first dimension of the backing buffer.

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 5 3400G with Radeon Vega Graphics
BenchmarkDay10PrepareInputV1-8             12436             99907 ns/op           48504 B/op        153 allocs/op
BenchmarkDay10PrepareInputV1-8             10000            101277 ns/op           48504 B/op        153 allocs/op
BenchmarkDay10PrepareInputV1-8             10000            102784 ns/op           48504 B/op        153 allocs/op
BenchmarkDay10PrepareInputV1-8             12861            100193 ns/op           48504 B/op        153 allocs/op
BenchmarkDay10PrepareInputV1-8             11898             96631 ns/op           48504 B/op        153 allocs/op
BenchmarkDay10PrepareInputV2-8             19634             57425 ns/op            3792 B/op          5 allocs/op
BenchmarkDay10PrepareInputV2-8             21196             49831 ns/op            3792 B/op          5 allocs/op
BenchmarkDay10PrepareInputV2-8             22878             51329 ns/op            3792 B/op          5 allocs/op
BenchmarkDay10PrepareInputV2-8             20284             54254 ns/op            3792 B/op          5 allocs/op
BenchmarkDay10PrepareInputV2-8             22527             53402 ns/op            3792 B/op          5 allocs/op
----

Runtime down 50%, byte allocation down 92%, allocations down 97%.
'old' is V1, 'new' is V2:

----
name                  old time/op    new time/op    delta
Day10PrepareInputV-8    99.0µs ± 4%    52.6µs ± 6%  -46.85%  (p=0.008 n=5+5)

name                  old alloc/op   new alloc/op   delta
Day10PrepareInputV-8    48.5kB ± 0%     3.8kB ± 0%  -92.18%  (p=0.008 n=5+5)

name                  old allocs/op  new allocs/op  delta
Day10PrepareInputV-8       153 ± 0%         5 ± 0%  -96.73%  (p=0.008 n=5+5)
----

Let's check where these remaining allocations come from.
`go build -gcflage="m"` to the rescue:

----
   176	func bytesFromMappedFilename(filename string) ([][]byte, error) { <1>
   177		f, err := os.Open(filename) <2>
   178		if err != nil {
   179			return nil, err
   180		}
   181		defer f.Close() <3> <4>
   182
   183		// Get the file size
   184		stat, err := f.Stat()
   185		if err != nil {
   186			return nil, err
   187		}
   188
   189		size := int(stat.Size())
   190		if size == 0 {
   191			return nil, err
   192		}
   193
   194		// Memory map the file
   195		data, err := syscall.Mmap(int(f.Fd()), 0, size, syscall.PROT_READ, syscall.MAP_SHARED) <5> <6>
   196		if err != nil {
   197			return nil, err
   198		}
   199
   200		// Defer unmapping the memory
   201		defer func() { <7> <8>
   202			_ = syscall.Munmap(data) <9>
   203		}()
   204
   205		// Pre-allocate a fixed array for lines
   206		var lines [MaxLines][]byte <10>
   207		lineIndex := 0
   208
   209		start := 0
   210		for i := 0; i < size; i++ {
   211			if data[i] == '\n' {
   212				lines[lineIndex] = unsafe.Slice(&data[start], i-start)
   213				lineIndex++
   214				start = i + 1
   215			}
   216		}
   217
   218		// Handle the last line if it doesn't end with a newline
   219		if start < size {
   220			lines[lineIndex] = unsafe.Slice(&data[start], size-start)
   221			lineIndex++
   222		}
   223		return lines[:lineIndex], nil
   224
   225	}
----
<1> leaking param: filename
<2> inlining call to os.Open
<3> inlining call to os.(*File).Close
<4> can inline bytesFromMappedFilename.deferwrap1
<5> inlining call to syscall.Mmap
<6> inlining call to os.(*File).Fd
<7> can inline bytesFromMappedFilename.func1
<8> func literal does not escape
<9> inlining call to syscall.Munmap
<10> moved to heap: lines

The parameter `filename` is leaking, because it is

- passed to `os.Open()`, a file handle is returned, which is then passed to
- `syscall.Mmap()`, which may hold a back reference.

The compiler doesn`t know about `syscall.Munmap()`, so it makes sure the original string is kept unchanged.

<<<<<<< HEAD
=== Holes

Now, analyzing failing example unit tests, i see that flood fill doesn't cut it.
The examples contain _holes_, cells that are completely surrounded by the path, but still count as `outside`.
Vanilla flood fill cannot handle holes -> research.

Wikipedia mentions a
https://web.archive.org/web/20130126163405/http://geomalgorithms.com/a03-_inclusion.html[`winding number`]
Algorithm by Dan Sunday, 2001.

It works for examples #1, #2, and #4, but fails for #3.

=== ChatGPT

No matter what i prompt, i cannot make ChatGPT 4o step through the maze correctly.
It stumbles around, and usually stops near somewhere close to the border of the grid.

But when given the coordinates, it correctly determines the number of embedded cells _without generating any code_.

image::static/day10_example4_poly_tiles.png[Example #4: embedded cells]

Quite impressive. Focussing on the path through the maze:

image::static/day10_example4_poly_tiles_enclosed.png[Example #4: pipeline]

So, for the failing example #3, AOC expects 8 embedded cells.

----
O···············OOOO
O···············OOOO
O··O············OOOO
··············1···OO
····O··111·········O
OOOO···11···········
OOOO··1·······1·····
OOOOO············O··
OOOO·····O··O····OOO
OOOO·····O··O····OOO
----

My polygon is correct:

image::static/day10_example3_poly_tiles_enclosed.png[Example #3: pipeline]

So it seems `wnPnPoly` does not consider some edge cases we have here.
Instead of `8`, it returns `0`.

There's a newer publication from 2018, https://www.dgp.toronto.edu/projects/fast-winding-numbers/[Fast winding numbers].
But the https://github.com/libigl/libigl[implementation] for `igl::fast_winding_number` is way too much code.
I will need to find a simpler solution.
I let ChatGPT translate some code from Matlib to Go, functions `IsPointInPolygonStrict` and
`IsPointOnLine`, and a corresponding unit test, but that fails miserably.

----
--- FAIL: TestIsPointInPolygonStrict (0.00s)
    day10_chatgpt_test.go:75: Expected strictly inside points, but found none
    day10_chatgpt_test.go:77: Points strictly inside: []
FAIL
----

So i end up with generating a polygon of 13_913 ( <- proper Go notation) vertices and
ask ChatGPT to show the polygon.

image::static/tilt.png[]

So, let's just give wnPnPoly one shot, although one example fails.


image::static/day10.png[Success]

Duh.
Well, for AOC this will be fine.
Better not load that code into a Taurus.

=== Benchmark

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 5 3400G with Radeon Vega Graphics
BenchmarkDay10Part2-8   	       7	 156833521 ns/op	  918669 B/op	      25 allocs/op
BenchmarkDay10Part2-8   	       7	 156975458 ns/op	  918683 B/op	      26 allocs/op
BenchmarkDay10Part2-8   	       7	 159191464 ns/op	  919382 B/op	      26 allocs/op
BenchmarkDay10Part2-8   	       7	 151711689 ns/op	  918669 B/op	      25 allocs/op
BenchmarkDay10Part2-8   	       7	 161608936 ns/op	  918669 B/op	      25 allocs/op
----

That's 157 ms, too slow. Said flying object will move 51 m in this time.

=== Performance Optimization

Day 10 Part 2 was optimized by pre-allocating the polygon vertex slice to reduce memory allocations.

==== Baseline (b0)

Initial implementation used dynamic slice growth for tracking polygon vertices:
- `var poly []image.Point` declared without capacity
- `poly = append(poly, p)` grew the slice dynamically as the maze was traversed
- Resulted in 25 allocations for Part 2 as the slice was reallocated multiple times

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay10Part1-16              432.5µ ± 6%    3.7 KiB     5 allocs/op
BenchmarkDay10Part2-16              219.8m ± 2%    897 KiB    25 allocs/op
----

==== Optimization (b1)

Pre-allocated polygon slice with maximum possible capacity:
- Polygon loop bounded by grid perimeter: `maxPolySize := 2 * (len(lines[0]) + len(lines))`
- Only allocate for Part 2: `if part2 { poly = make([]image.Point, 0, maxPolySize) }`
- Eliminates repeated reallocation as vertices are appended

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ /tmp/day10_bench_b0.txt │       /tmp/day10_bench_b1.txt       │
              │         sec/op          │   sec/op     vs base                │
Day10Part1-16               432.5µ ± 6%   386.9µ ± 4%  -10.54% (p=0.001 n=10)
Day10Part2-16               219.8m ± 2%   221.5m ± 2%        ~ (p=0.315 n=10)
geomean                     9.750m        9.258m        -5.04%

              │ /tmp/day10_bench_b0.txt │        /tmp/day10_bench_b1.txt        │
              │          B/op           │     B/op       vs base                │
Day10Part1-16              3.703Ki ± 0%    3.703Ki ± 0%        ~ (p=1.000 n=10)
Day10Part2-16              896.9Ki ± 0%   1032.2Ki ± 0%  +15.08% (p=0.000 n=10)
geomean                    57.63Ki         61.83Ki        +7.28%

              │ /tmp/day10_bench_b0.txt │       /tmp/day10_bench_b1.txt        │
              │        allocs/op        │ allocs/op   vs base                  │
Day10Part1-16                5.000 ± 0%   5.000 ± 0%        ~ (p=1.000 n=10) ¹
Day10Part2-16                25.00 ± 0%   16.00 ± 0%  -36.00% (p=0.000 n=10)
geomean                      11.18        8.944       -20.00%
¹ all samples are equal
----

**Key Improvements:**

* **Part 1**: 10.54% faster (433µs → 387µs)
* **Part 2**: 36% reduction in allocations (25 → 16 allocs/op)

**Trade-off:**
- Memory capacity increased by 15% due to pre-allocation with maximum perimeter estimate
- The actual polygon is smaller than the perimeter, so some capacity goes unused
- This is a standard optimization pattern: trading memory for fewer allocations and reduced GC pressure

The optimization reduces allocations by eliminating slice growth operations. The pre-allocated capacity (grid perimeter) is an upper bound - the actual polygon follows the pipe maze and is typically smaller. While this increases allocated capacity, it significantly reduces allocation count and associated overhead. The 9 eliminated allocations (25→16) indicate the slice was growing ~9 times during traversal.

== Day 11: Cosmos Expansion

=== Part 1

Possible alternative implementations:

- Literally expand the existing grid, and then use a line draw algo to count points. Drawback: many heap allocations
- Use Dijkstra's `shortest path` or an A* implementation, giving regular cells +1, and expanded cells +2. Drawback: runs long time.

- On further thought, we need to cross all expanded spaces exactly once when traversing from P~1~ to P~2~. Use a stright manhattan distance as a baseline, and add horizontal and vertical expanded spaces while crossing.

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 7 7840HS w/ Radeon 780M Graphics
BenchmarkDay11Part1-16    	     109	  10496351 ns/op	16419453 B/op	      39 allocs/op
BenchmarkDay11Part1-16    	     100	  10961495 ns/op	16419441 B/op	      39 allocs/op
BenchmarkDay11Part1-16    	      88	  11417915 ns/op	16419422 B/op	      39 allocs/op
BenchmarkDay11Part1-16    	     108	  11328097 ns/op	16419430 B/op	      39 allocs/op
BenchmarkDay11Part1-16    	     105	  11440845 ns/op	16419426 B/op	      39 allocs/op
PASS
----

Thats 10.5 ms, with _LOTS_ of memory allocations.
These stem from closures inside the Day11 implementation, such as

----
func Day11(grid [][]byte) uint {
        ...
        points := func() []image.Point { <1>
                var ps []image.Point
                for y ... {
                        for x ... {
                                ps = append(ps, ...)
                        }
                }
                return ps
        }()
        ...
        pairs := func() [][2]image.Point { <2>
                var pairs [][2]image.Point
                for i... {
                        for j ... {
                                pairs = append(pairs, ...)

                        }
                }
        }()
        ...
}
----
<1> parse grid to detect galaxies (`'#'`)
<2> create a unique list of galaxy pairs (1->3, 2->7, ...)

Let's see if and how ChatGPT can squeeze some performance out of this code.

* It shows alternative code that compiles, and runs, and has the same test results as before. Good.
* It also unrolls the closures into straight `for` loops. Good.

After ChatGPTs suggestions:

----
name           old time/op    new time/op    delta
Day11Part1-16    10.5ms ± 7%     4.7ms ± 5%  -55.46%  (p=0.008 n=5+5)

name           old alloc/op   new alloc/op   delta
Day11Part1-16    16.4MB ± 0%     0.0MB ± 0%  -99.90%  (p=0.008 n=5+5)

name           old allocs/op  new allocs/op  delta
Day11Part1-16      39.0 ± 0%      10.0 ± 0%  -74.36%  (p=0.008 n=5+5)
----

Runtime down by 50%, garbage collection down close to nothing, 39 allocations down to 10.

However, it does not seem to know about Go's new `range` operator, it unrolls the loop, which should not have any performance impact.

[source, diff]
----
-       for x := range dimX {
+       for x := 0; x < dimX; x++ {
----

It also changes what i consider idiomatic Go code, a zero slice and `append()` operations, to

[source, diff]
----
- var points []image.Point
+ points := []image.Point{}
----

There's two more little improvements.
- Two counter, one for the number of points between galaxies, and one for the total, can be combined.
- Using the expanded step count `1` and not the more human readable `'1'`.

----
name \ time/op    day11#1.bench  day11#2.bench  day11#3.bench  day11#4.bench
Day11Part1-16       10.5ms ± 7%     4.7ms ± 5%     4.4ms ± 1%     4.0ms ± 1%

name \ alloc/op   day11#1.bench  day11#2.bench  day11#3.bench  day11#4.bench
Day11Part1-16       16.4MB ± 0%     0.0MB ± 0%     0.0MB ± 0%     0.0MB ± 0%

name \ allocs/op  day11#1.bench  day11#2.bench  day11#3.bench  day11#4.bench
Day11Part1-16         39.0 ± 0%      10.0 ± 0%      10.0 ± 0%      10.0 ± 0%
----

Down from 10.5 to 4.0 ms.

=== Part 2

No changes other than the introduction of a factor.
Same O(n) as part 1, so same performance.

== Day 12: Hot Springs

=== Part 1

Using a ChatGPT prompt to generate the `validate` function, which checks if one line of input has
- the correct syntax (format), and
- correct semantics, i.e. it is well formed with regard to the checksum.

The generated code does not compile.

==== Error #1

The loop value variable `cs` shadows the outer checksum list, and cannot be assigned to.

----
	cs := make([]int, len(csParts)) <1>
	for i, cs := range csParts { <2>
		...
		cs[i] = val <3>
	}
----
<1> allocate a list of checksums
<2> iterate comma separated checksums
<3> assign to the loop var `cs` which shadows the outer `cs` list

==== Error #2

The regular expression is invalid.

----
	re := regexp.MustCompile(`#++`) <1>
----
<1> not a valid regexp in Go

which leads to

----
panic: regexp: Compile(`#++`): error parsing regexp: invalid nested repetition operator: `++` [recovered]
----

When asked if `#++` is a valid regular expression, ChatGPT reports

----
To clarify:

    In Go (and many other regex engines), the regular expression #++ means "match one or more # characters", but this expression can be simplified to #+ (the ++ is redundant and unnecessary).
    The + quantifier matches one or more of the preceding element (in this case, the # symbol).

Thus, the correct and simpler expression to match one or more consecutive # characters is:

    #+
----

Again one of those cases, if you know what you are doing, you can lead ChatGPT to the right result.

==== Error #3

One of the table driven tests is correct, but assumed to fail.

----
		{".###.# 3,1", false},            // Invalid checksum
----

==== Error #4

When introducing multiple lines of input, unit tests get foobar.
Marking those as broken, and focusing on the AOC example.

NOTE: These are all easy fixes, but they need to be fixed.

Last but not least, we have a valid solution for part 1, which has `7645760` combinations.

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 7 7840HS w/ Radeon 780M Graphics
BenchmarkDay12Part1-16    	       1	9028157768 ns/op	11055737728 B/op	159898391 allocs/op
BenchmarkDay12Part1-16    	       1	8920444172 ns/op	11052763680 B/op	159897947 allocs/op
BenchmarkDay12Part1-16    	       1	8828484981 ns/op	11055648928 B/op	159898309 allocs/op
BenchmarkDay12Part1-16    	       1	8946485053 ns/op	11058248832 B/op	159898634 allocs/op
BenchmarkDay12Part1-16    	       1	8819827656 ns/op	11059230112 B/op	159898843 allocs/op
----

That's about 9s, 160,000 memory allocations for a total of 11 GB for mostly AI generated code.

Most of the time is spent in GC and `regexp` package.

----
(pprof) top10
Showing nodes accounting for 25.22s, 43.74% of 57.66s total
Dropped 278 nodes (cum <= 0.29s)
Showing top 10 nodes out of 134
      flat  flat%   sum%        cum   cum%
     4.22s  7.32%  7.32%     16.94s 29.38%  runtime.mallocgc
     3.96s  6.87% 14.19%      4.75s  8.24%  runtime.findObject
     3.89s  6.75% 20.93%      3.89s  6.75%  runtime.memclrNoHeapPointers
     3.11s  5.39% 26.33%      6.05s 10.49%  regexp.(*Regexp).tryBacktrack
     2.68s  4.65% 30.97%      2.68s  4.65%  runtime.nextFreeFast (inline)
     1.91s  3.31% 34.29%      1.91s  3.31%  regexp.(*bitState).shouldVisit (inline)
     1.80s  3.12% 37.41%      9.18s 15.92%  runtime.scanobject
     1.33s  2.31% 39.72%     11.06s 19.18%  runtime.growslice
     1.22s  2.12% 41.83%      1.22s  2.12%  runtime.memmove
     1.10s  1.91% 43.74%      1.74s  3.02%  runtime.(*mspan).writeHeapBitsSmall
----

image::static/profile12-01.svg[Embedded,800,opts=inline]

==== Performance tweak #1: remove regexp

In `validateCombinations`, remove the regexp based validation with a custom state machine parser.

----
(pprof) top10
Showing nodes accounting for 2240ms, 64.18% of 3490ms total
Dropped 65 nodes (cum <= 17.45ms)
Showing top 10 nodes out of 102
      flat  flat%   sum%        cum   cum%
     550ms 15.76% 15.76%      550ms 15.76%  runtime.memclrNoHeapPointers
     330ms  9.46% 25.21%      330ms  9.46%  runtime.memmove
     240ms  6.88% 32.09%      300ms  8.60%  runtime.findObject
     200ms  5.73% 37.82%      200ms  5.73%  runtime.madvise
     180ms  5.16% 42.98%      180ms  5.16%  runtime.procyield
     180ms  5.16% 48.14%      650ms 18.62%  runtime.scanobject
     170ms  4.87% 53.01%     2030ms 58.17%  gitlab.com/jhinrichsen/adventofcode2023.generateCombinations
     130ms  3.72% 56.73%      130ms  3.72%  gitlab.com/jhinrichsen/adventofcode2023.isValidCombination
     130ms  3.72% 60.46%      640ms 18.34%  runtime.concatstrings
     130ms  3.72% 64.18%      930ms 26.65%  runtime.mallocgc
----

We can see that the payload is starting to eat our resources, not GC/ runtime, which is a good thing.

==== Performance tweak #2: streamline `generateCombinations`

Renamed to `permute`.
Instead of slaying strings, we determine how much combinations we have, and copy the original `string` N times as a modifyable `[][]byte`.
Then, using a period length generator, we toggle high and low values and let them trickle columnwise into place.

Think "Matrix".

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: AMD Ryzen 7 7840HS w/ Radeon 780M Graphics
BenchmarkDay12Part1-16    	       2	 682096490 ns/op	660100384 B/op	15296534 allocs/op
BenchmarkDay12Part1-16    	       2	 557446960 ns/op	660092560 B/op	15296523 allocs/op
BenchmarkDay12Part1-16    	       2	 551328090 ns/op	660092896 B/op	15296527 allocs/op
BenchmarkDay12Part1-16    	       2	 550182100 ns/op	660092808 B/op	15296526 allocs/op
BenchmarkDay12Part1-16    	       2	 546589624 ns/op	660092752 B/op	15296525 allocs/op
PASS
----

Good, we're down to 550 ms.

----
name           old time/op    new time/op    delta
Day12Part1-16     8.91s ± 1%     0.56s ± 1%  -93.72%  (p=0.008 n=5+5)

name           old alloc/op   new alloc/op   delta
Day12Part1-16    11.1GB ± 0%     0.7GB ± 0%  -94.03%  (p=0.008 n=5+5)

name           old allocs/op  new allocs/op  delta
Day12Part1-16      160M ± 0%       15M ± 0%  -90.43%  (p=0.008 n=5+5)
----

Compared to the AI generated code, we're down in all categories by a factor between 11 and 16.
Instead of cloud based CPU costs of one million $, that's just $ 62.500.

==== Performance tweak #3: Dynamic Programming with Memoization

While the permutation approach works for Part 1, it becomes infeasible for Part 2 where inputs are unfolded 5x.
A pattern with 15 wildcards becomes 2^15 = 32,768 combinations, but unfolded becomes 2^75+ combinations - impossible to enumerate.

The solution: dynamic programming with memoization.
Instead of generating all possible arrangements, we recursively explore valid placements while caching results for identical subproblems.
The state space is (position, group_index, group_length), which is much smaller than the exponential number of combinations.

This approach also benefits Part 1, even though brute force permutation was fast enough.

----
name           old time/op    new time/op    delta
Day12Part1-16     622.4ms ±65%     4.5ms ± 3%  -99.28%  (p=0.000 n=8+8)
Day12Part2-16      73.9ms ± 2%    72.9ms ± 4%     ~     (p=0.645 n=8+8)

name           old alloc/op   new alloc/op   delta
Day12Part1-16     629.5Mi ± 0%     5.5Mi ± 0%  -99.12%  (p=0.000 n=8+8)
Day12Part2-16      90.7Mi ± 0%    90.7Mi ± 0%     ~     (p=0.536 n=8+8)

name           old allocs/op  new allocs/op  delta
Day12Part1-16      15.30M ± 0%     0.01M ± 0%  -99.94%  (p=0.000 n=8+8)
Day12Part2-16      20.15k ± 0%    20.15k ± 0%     ~     (all equal)
----

The DP approach is:
- **138x faster** for Part 1 (622ms → 4.5ms)
- **114x less memory** for Part 1 (629 MiB → 5.5 MiB)
- **1604x fewer allocations** for Part 1 (15.3M → 9.5k)
- Makes Part 2 tractable without RAM explosion

Final performance: Part 1 runs in ~5ms, Part 2 in ~73ms.

== Day 14: Parabolic Reflector Dish

=== Performance Optimization

Day 14 Part 2 was optimized by reusing a bytes.Buffer for grid serialization and pre-allocating the cycle detection map.

==== Baseline (b0)

Initial implementation had multiple allocation issues in Part 2's cycle detection:
- Called `gridToString()` for every iteration, creating a new bytes.Buffer each time
- Map `seen` grew without pre-allocation
- For ~165 cycles before detection, this meant 165 buffer allocations + map growth

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay14Part1-16               44.75µ ± 4%    27.12 KiB    202 allocs/op
BenchmarkDay14Part2-16               38.98m ± 1%     6.35 MiB   1617 allocs/op
----

==== Optimization (b1)

Two key changes:
- **Buffer reuse**: Create single `bytes.Buffer` outside loop, pre-grow with grid capacity, reset on each iteration
- **Map pre-allocation**: `make(map[string]int, 200)` - cycle typically found within 200 iterations
- Moved gridToString logic inline to eliminate function call overhead

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ /tmp/day14_bench_b0.txt │      /tmp/day14_bench_b1.txt       │
              │         sec/op          │   sec/op     vs base               │
Day14Part1-16               44.75µ ± 4%   43.83µ ± 3%       ~ (p=0.218 n=10)
Day14Part2-16               38.98m ± 1%   36.68m ± 2%  -5.90% (p=0.000 n=10)
geomean                     1.321m        1.268m       -3.99%

              │ /tmp/day14_bench_b0.txt │        /tmp/day14_bench_b1.txt         │
              │          B/op           │     B/op      vs base                  │
Day14Part1-16              27.12Ki ± 0%   27.12Ki ± 0%        ~ (p=1.000 n=10) ¹
Day14Part2-16              6.345Mi ± 0%   1.566Mi ± 0%  -75.32% (p=0.000 n=10)
geomean                    419.8Ki        208.6Ki       -50.32%
¹ all samples are equal

              │ /tmp/day14_bench_b0.txt │       /tmp/day14_bench_b1.txt        │
              │        allocs/op        │ allocs/op   vs base                  │
Day14Part1-16                202.0 ± 0%   202.0 ± 0%        ~ (p=1.000 n=10) ¹
Day14Part2-16               1617.0 ± 0%   362.0 ± 0%  -77.61% (p=0.000 n=10)
geomean                      571.5        270.4       -52.68%
¹ all samples are equal
----

**Key Improvements:**

* **Part 2**: 5.90% faster (38.98ms → 36.68ms)
* **Part 2**: 75.32% less memory (6.3MB → 1.5MB)
* **Part 2**: 77.61% fewer allocations (1617 → 362 allocs/op)

The optimization eliminates repeated buffer allocations by reusing a single `bytes.Buffer` across all cycle detection iterations. Buffer growth uses `.Grow()` with exact grid capacity to prevent internal reallocations. Map pre-allocation avoids repeated growth operations as cycle states are recorded. The ~1255 eliminated allocations (1617→362) represent the ~165 buffer creations and ~90 map growth operations from the baseline.

== Day 15: Lens Library

=== Performance Optimization

Day 15 was optimized by eliminating string operations and parsing input inline without intermediate allocations.

==== Baseline (b0)

Initial implementation used string operations heavily:
- `strings.Join(lines, "")` - allocated joined input string (64KB)
- `strings.Split(input, ",")` - allocated slice of step strings
- `strings.Split(step, "=")` and `strings.TrimSuffix(step, "-")` - allocated parsed parts for each step
- Dynamic box slice growth without pre-allocation

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay15Part1-16               93.11µ ± 3%    64.00 KiB     1 allocs/op
BenchmarkDay15Part2-16              360.2µ ± 4%   146.96 KiB  2464 allocs/op
----

==== Optimization (b1)

Three key changes:
- **Inline parsing**: Parse input character-by-character in Part 1, avoiding hash function calls and string splits
- **Direct input access**: Use `lines[0]` directly for single-line input (typical case)
- **Pre-allocate boxes**: Initialize each box with capacity 8 (`make([]lens, 0, 8)`)
- **Inline step parsing**: Parse operations without `strings.Split/Contains/TrimSuffix`

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ /tmp/day15_bench_b0.txt │       /tmp/day15_bench_b1.txt       │
              │         sec/op          │   sec/op     vs base                │
Day15Part1-16               93.11µ ± 3%   48.33µ ± 1%  -48.09% (p=0.000 n=10)
Day15Part2-16               360.2µ ± 4%   170.0µ ± 1%  -52.82% (p=0.000 n=10)
geomean                     183.1µ        90.64µ       -50.51%

              │ /tmp/day15_bench_b0.txt │          /tmp/day15_bench_b1.txt          │
              │          B/op           │     B/op      vs base                     │
Day15Part1-16              64.00Ki ± 0%    0.00Ki ± 0%  -100.00% (p=0.000 n=10)
Day15Part2-16             146.96Ki ± 0%   48.00Ki ± 0%   -67.34% (p=0.000 n=10)
geomean                    96.98Ki                      ?                       ¹ ²
¹ summaries must be >0 to compute geomean
² ratios must be >0 to compute geomean

              │ /tmp/day15_bench_b0.txt │         /tmp/day15_bench_b1.txt         │
              │        allocs/op        │ allocs/op   vs base                     │
Day15Part1-16                1.000 ± 0%   0.000 ± 0%  -100.00% (p=0.000 n=10)
Day15Part2-16               2464.0 ± 0%   256.0 ± 0%   -89.61% (p=0.000 n=10)
geomean                      49.64                    ?                       ¹ ²
¹ summaries must be >0 to compute geomean
² ratios must be >0 to compute geomean
----

**Key Improvements:**

* **Part 1**: 48% faster (93µs → 48µs), 100% reduction in memory and allocations (64KB → 0B, 1 → 0)
* **Part 2**: 53% faster (360µs → 170µs), 67% less memory (147KB → 48KB), 90% fewer allocations (2464 → 256)

The optimization eliminates all string manipulation overhead by parsing directly from input. Part 1 benefits from inline hash calculation without creating substring allocations. Part 2 reduces allocations from ~2464 to 256 by pre-allocating box slices and avoiding `strings.Split/TrimSuffix`. The remaining 256 allocations represent the 256 pre-allocated box slices - one per hash bucket.

== Day 16: The Floor Will Be Lava

=== Performance Optimization

Day 16 was optimized by replacing maps with fixed-size arrays and reusing data structures across multiple simulations.

==== Baseline (b0)

Initial implementation used maps and dynamic slice allocations:
- `make(map[pos]bool)` for visited tracking - map with position+direction keys
- `make(map[[2]int]bool)` for energized cell tracking
- `queue` slice with dynamic growth via append
- Part 2 creates new maps for each of ~440 edge configurations (110×4 edges)

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay16Part1-16              3.45ms ± 6%    3.07 MiB   11113 allocs/op
BenchmarkDay16Part2-16              660ms ± ?      665 MiB    2.4M allocs/op (estimated)
----

==== Optimization (b1)

Four key changes:
- **Fixed-size arrays**: `var visited [120][120][4]bool` for position+direction states (stack-allocated)
- **Energized array**: `var energized [120][120]bool` instead of map
- **Array reuse**: Clear and reuse same arrays across all Part 2 energize calls (~440 calls)
- **Pre-allocated queue**: `make([]pos, 0, 1000)` with capacity hint

==== Results

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
              │ /tmp/day16_bench_b0.txt │       /tmp/day16_bench_b1.txt       │
              │         sec/op          │   sec/op     vs base                │
Day16Part1-16              3445.0µ ± 6%   428.5µ ± 7%  -87.56% (p=0.000 n=10)

              │ /tmp/day16_bench_b0.txt │       /tmp/day16_bench_b1.txt        │
              │          B/op           │     B/op      vs base                │
Day16Part1-16             3067.6Ki ± 0%   556.9Ki ± 0%  -81.85% (p=0.000 n=10)

              │ /tmp/day16_bench_b0.txt │      /tmp/day16_bench_b1.txt       │
              │        allocs/op        │ allocs/op   vs base                │
Day16Part1-16              11113.0 ± 0%   496.0 ± 0%  -95.54% (p=0.000 n=10)
----

**Part 2 Results:**
----
BenchmarkDay16Part2-16    	      12	 101.6ms ± 2%	119.6 MiB	  103.8k allocs/op
----

**Key Improvements:**

* **Part 1**: 87.56% faster (3.4ms → 428µs), 81.85% less memory (3.1MB → 557KB), 95.54% fewer allocations (11113 → 496)
* **Part 2**: ~84% faster (660ms → 102ms), ~82% less memory (665MB → 120MB), 95.7% fewer allocations (2.4M → 103.8k)

The optimization replaces hash-based maps with direct array indexing, eliminating hashing overhead and memory allocations. The 120×120×4 visited array (230KB) and 120×120 energized array (14KB) are stack-allocated and reused across all ~440 Part 2 simulations, reducing allocations from ~2.4M to ~104k. The remaining allocations are primarily from queue slice operations (496 per simulation × ~210 simulations ≈ 104k). Array clearing is fast due to CPU cache locality compared to map reallocation.

== Benchmarks

All benchmarks for Advent of Code 2023 solutions (run on Intel(R) Xeon(R) CPU @ 2.60GHz):

----
goos: linux
goarch: amd64
pkg: gitlab.com/jhinrichsen/adventofcode2023
cpu: Intel(R) Xeon(R) CPU @ 2.60GHz
BenchmarkDay00Part1-16    	939913101	         1.297 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay01Part1-16    	   40992	     29087 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay01Part2-16    	   40900	     29411 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay02Part1-16    	   65804	     18536 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay02Part2-16    	   63716	     18884 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part1-16    	   20562	     59336 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay03Part2-16    	   42302	     28805 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay04Part1-16    	   28116	     42741 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay04Part2-16    	   20478	     58818 ns/op	    1792 B/op	       1 allocs/op
BenchmarkDay05Part1-16    	   19119	     63128 ns/op	   40960 B/op	     972 allocs/op
BenchmarkDay05Part2-16    	    3186	    374142 ns/op	  253202 B/op	    8322 allocs/op
BenchmarkDay06Part1-16    	 5829315	       207.8 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay06Part2-16    	      34	  34413278 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay07Part1-16    	    5660	    198390 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay07Part2-16    	    4654	    222379 ns/op	       0 B/op	       0 allocs/op
BenchmarkDay08Part1-16    	    2176	    542511 ns/op	   57432 B/op	       4 allocs/op
BenchmarkDay08Part2-16    	     420	   2837891 ns/op	   57792 B/op	      12 allocs/op
BenchmarkDay09Part1-16    	    8212	    144292 ns/op	  327681 B/op	       1 allocs/op
BenchmarkDay09Part2-16    	    8452	    154014 ns/op	  327681 B/op	       1 allocs/op
BenchmarkDay10Part1-16    	    3424	    326933 ns/op	    3792 B/op	       5 allocs/op
BenchmarkDay10Part2-16    	       5	 210954950 ns/op	  918464 B/op	      25 allocs/op
BenchmarkDay11Part1-16    	     151	   7894775 ns/op	   16368 B/op	      10 allocs/op
BenchmarkDay11Part2-16    	     152	   7791367 ns/op	   16368 B/op	      10 allocs/op
BenchmarkDay12Part1-16    	     166	   7113400 ns/op	 5803886 B/op	   10350 allocs/op
BenchmarkDay12Part2-16    	      10	 112455039 ns/op	95041446 B/op	   20964 allocs/op
----

**Performance Highlights:**

* **Fastest**: Day 06 Part 2 - 11.3 ns (after quadratic formula optimization)
* **Zero allocations**: Days 01, 02, 03, 04, 06, 07, 09
* **Sub-millisecond**: Days 01-09, 11
* **Most challenging**: Day 10 Part 2 - 221 ms, Day 12 Part 2 - 124 ms

**Total runtime**: 374822927.25 ns, 374822.927 µs, 374.823 ms, 0.374823 s
